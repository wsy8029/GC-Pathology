{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0af1dae",
   "metadata": {},
   "source": [
    "",
    "",
    "# Histopathology Pipeline Prototype (2024년 biopsy matches)",
    "",
    "",
    "",
    "이 노트북은 2024년 biopsy matching 데이터셋을 탐색하고 histopathology classifier 학습을 위한 준비 과정을 다룹니다. 여기서는 raw diagnostic reports를 machine-readable labels로 변환하고 `.svs` whole-slide images (WSI)를 위한 patch-extraction workflow를 개괄하는 데 초점을 맞춥니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c938b",
   "metadata": {},
   "source": [
    "",
    "",
    "## 목표",
    "",
    "",
    "",
    "1. Excel/CSV에서 export된 biopsy metadata를 점검하고 정제합니다.",
    "",
    "2. 이질적인 diagnostic 텍스트를 일관된 disease labels로 통합합니다.",
    "",
    "3. 이용 가능한 annotations로 구현 가능한 modelling target을 정의합니다.",
    "",
    "4. downstream modelling을 위해 stratified train/validation/test splits를 생성합니다.",
    "",
    "5. `.svs` slides에 대한 patch extraction workflow를 개괄하고, 누락된 파일을 확인하며 기본 tissue filtering heuristic을 서술합니다.",
    "",
    "",
    "",
    "> **참고:** 현재 레포지토리에는 metadata CSV만 포함되어 있습니다. 실제 WSI `.svs` 파일은 patch extractor 셀을 실행하기 전에 별도로 마운트해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86851db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_PATH = Path('Data') / '조직검사 결과 매칭(2024)_utf8_pruned.csv'\n",
    "assert DATA_PATH.exists(), f\"Missing dataset: {DATA_PATH}\"\n",
    "\n",
    "raw_df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Loaded {len(raw_df):,} biopsy records with {raw_df.shape[1]} columns\")\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76059c2",
   "metadata": {},
   "source": [
    "### 결측값 개요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ca6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing_summary = (\n",
    "    raw_df.isna()\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .to_frame(name='missing_ratio')\n",
    ")\n",
    "missing_summary.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078be220",
   "metadata": {},
   "source": [
    "",
    "",
    "## Diagnosis 정규화",
    "",
    "",
    "",
    "`DIAGNOSIS` 컬럼에는 장문의 free-text 문자열이 포함되어 있습니다. 이를 machine learning target으로 사용하기 위해서는 다음과 같이 처리합니다:",
    "",
    "",
    "",
    "1. 첫 번째 쉼표 이전 부분만 유지하여 staging이나 margin 코멘트를 제거합니다.",
    "",
    "2. 괄호를 제거하고 공백을 정규화합니다.",
    "",
    "3. 소문자로 변환하고 자주 등장하는 동의어를 canonical form으로 매핑합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5aeeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NORMALISATION_RULES = [\n",
    "    (r'^mast cell tumor', 'mast cell tumor'),\n",
    "    (r'^cutaneous mast cell tumor', 'mast cell tumor'),\n",
    "    (r'^subcutaneous mast cell tumor', 'mast cell tumor'),\n",
    "    (r'^mammary gland adenoma', 'mammary adenoma'),\n",
    "    (r'^mammary complex adenoma', 'mammary complex adenoma'),\n",
    "    (r'^mammary benign mixed tumor', 'mammary benign mixed tumor'),\n",
    "    (r'^mammary carcinoma', 'mammary carcinoma'),\n",
    "    (r'^mammary duct carcinoma', 'mammary carcinoma'),\n",
    "    (r'^mammary adenoma', 'mammary adenoma'),\n",
    "    (r'^lipoma', 'lipoma'),\n",
    "    (r'^subcutaneous lipoma', 'lipoma'),\n",
    "    (r'^hepatic lipoma', 'lipoma'),\n",
    "    (r'^sebaceous adenoma', 'sebaceous adenoma'),\n",
    "    (r'^sebaceous epithelioma', 'sebaceous epithelioma'),\n",
    "    (r'^soft tissue sarcoma', 'soft tissue sarcoma'),\n",
    "    (r'^trichoblastoma', 'trichoblastoma'),\n",
    "]\n",
    "\n",
    "\n",
    "def normalise_diagnosis(value: str) -> str:\n",
    "    if not isinstance(value, str) or not value.strip():\n",
    "        return 'unknown'\n",
    "\n",
    "    base = value.split(',')[0]\n",
    "    base = re.sub(r'\\([^)]*\\)', '', base)\n",
    "    base = re.sub(r'[^a-zA-Z0-9\\s]', ' ', base)\n",
    "    base = re.sub(r'\\s+', ' ', base).strip().lower()\n",
    "\n",
    "    for pattern, canonical in NORMALISATION_RULES:\n",
    "        if re.match(pattern, base):\n",
    "            return canonical\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "raw_df['disease_family'] = raw_df['DIAGNOSIS'].map(normalise_diagnosis)\n",
    "print('Unique disease families:', raw_df['disease_family'].nunique())\n",
    "raw_df['disease_family'].value_counts().head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138be111",
   "metadata": {},
   "source": [
    "",
    "",
    "### 모델링 target 선택",
    "",
    "",
    "",
    "이 데이터셋에는 수천 개의 diagnostic 문구가 포함되어 있습니다. 5,000개가 넘는 라벨 전체를 대상으로 multi-class classifier를 학습하면 극도로 희소해집니다. 따라서 빈도가 낮은 클래스는 `other` 버킷으로 모으고, 가장 흔한 disease families에 집중합니다. 클래스 균형을 조정하려면 `MIN_CASES` 값을 수정하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a6042",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MIN_CASES = 250\n",
    "value_counts = raw_df['disease_family'].value_counts()\n",
    "major_labels = value_counts[value_counts >= MIN_CASES].index.tolist()\n",
    "print(f\"Keeping {len(major_labels)} frequent labels (≥{MIN_CASES} cases)\")\n",
    "\n",
    "raw_df['target_label'] = np.where(\n",
    "    raw_df['disease_family'].isin(major_labels),\n",
    "    raw_df['disease_family'],\n",
    "    'other'\n",
    ")\n",
    "\n",
    "raw_df['target_label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2144dc",
   "metadata": {},
   "source": [
    "",
    "",
    "### Train/validation/test split 메타데이터",
    "",
    "",
    "",
    "통합된 target labels를 기준으로 stratified splits를 생성합니다. 이후 추출된 patch-level 데이터와 조인하여 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "META_COLUMNS = ['INSP_RQST_NO', 'FOLDER', 'FILE_NAME', 'target_label']\n",
    "meta_df = raw_df[META_COLUMNS].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    meta_df,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=meta_df['target_label']\n",
    ")\n",
    "valid_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=temp_df['target_label']\n",
    ")\n",
    "\n",
    "print('Train size:', len(train_df))\n",
    "print('Valid size:', len(valid_df))\n",
    "print('Test size :', len(test_df))\n",
    "\n",
    "split_summary = {\n",
    "    'train': train_df['target_label'].value_counts(normalize=True).to_dict(),\n",
    "    'valid': valid_df['target_label'].value_counts(normalize=True).to_dict(),\n",
    "    'test': test_df['target_label'].value_counts(normalize=True).to_dict(),\n",
    "}\n",
    "json.dumps(split_summary, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b0caa9",
   "metadata": {},
   "source": [
    "",
    "",
    "## WSI 가용성 확인",
    "",
    "",
    "",
    "metadata는 요청별 `FOLDER`에 저장된 `.svs` 파일을 참조합니다. slides가 마운트된 디렉터리 경로를 `WSI_ROOT`에 맞게 수정하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WSI_ROOT = Path('Data/WSI')  # TODO: update to actual location\n",
    "\n",
    "if not WSI_ROOT.exists():\n",
    "    print(f\"WSI root missing at {WSI_ROOT.resolve()}. Patch extraction will be skipped.\")\n",
    "else:\n",
    "    sample_rows = raw_df.head(3)\n",
    "    for _, row in sample_rows.iterrows():\n",
    "        candidate = WSI_ROOT / row['FOLDER'] / row['FILE_NAME']\n",
    "        print(candidate, 'exists' if candidate.exists() else 'missing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9dd9ba",
   "metadata": {},
   "source": [
    "",
    "",
    "## Patch extraction 유틸리티",
    "",
    "",
    "",
    "고해상도 패치를 스트리밍하기 위해 [OpenSlide](https://openslide.org/)를 사용합니다. 다음 셀을 실행하기 전에 prerequisites를 설치하세요:",
    "",
    "",
    "",
    "```bash",
    "",
    "sudo apt-get install openslide-tools",
    "",
    "pip install openslide-python",
    "",
    "```",
    "",
    "",
    "",
    "OpenSlide을 사용할 수 없는 경우 아래 코드는 우회 동작하도록 작성되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f841a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import openslide\n",
    "    from PIL import Image\n",
    "    HAS_OPENSLIDE = True\n",
    "    print('OpenSlide version:', openslide.__library_version__)\n",
    "except Exception as exc:  # noqa: BLE001\n",
    "    HAS_OPENSLIDE = False\n",
    "    print('OpenSlide not available:', exc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f9e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class PatchSpec:\n",
    "    slide_id: str\n",
    "    x: int\n",
    "    y: int\n",
    "    level: int\n",
    "    size: int\n",
    "    label: str\n",
    "    source_path: Path\n",
    "\n",
    "\n",
    "def resolve_slide_path(row: pd.Series, root: Path) -> Optional[Path]:\n",
    "    \"\"\"Resolve the absolute slide path using folder/name hints.\"\"\"\n",
    "    candidates = [root / row['FOLDER'] / row['FILE_NAME'], root / row['FILE_NAME']]\n",
    "    for cand in candidates:\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "\n",
    "def tissue_fraction(tile: Image.Image) -> float:\n",
    "    arr = np.asarray(tile.convert('L'))\n",
    "    norm = (arr - arr.min()) / (arr.ptp() + 1e-6)\n",
    "    return float((norm < 0.85).mean())\n",
    "\n",
    "\n",
    "def extract_patches_for_slide(\n",
    "    slide_path: Path,\n",
    "    label: str,\n",
    "    level: int = 0,\n",
    "    patch_size: int = 256,\n",
    "    stride: Optional[int] = None,\n",
    "    max_patches: int = 200,\n",
    "    tissue_threshold: float = 0.2,\n",
    ") -> List[PatchSpec]:\n",
    "    if not HAS_OPENSLIDE:\n",
    "        raise RuntimeError('OpenSlide support is required to extract patches.')\n",
    "\n",
    "    slide = openslide.OpenSlide(str(slide_path))\n",
    "    stride = stride or patch_size\n",
    "    width, height = slide.level_dimensions[level]\n",
    "\n",
    "    patches: List[PatchSpec] = []\n",
    "    for y in range(0, height - patch_size + 1, stride):\n",
    "        for x in range(0, width - patch_size + 1, stride):\n",
    "            region = slide.read_region((x, y), level, (patch_size, patch_size))\n",
    "            if tissue_fraction(region) < tissue_threshold:\n",
    "                continue\n",
    "            patches.append(PatchSpec(\n",
    "                slide_id=slide_path.stem,\n",
    "                x=x,\n",
    "                y=y,\n",
    "                level=level,\n",
    "                size=patch_size,\n",
    "                label=label,\n",
    "                source_path=slide_path,\n",
    "            ))\n",
    "            if len(patches) >= max_patches:\n",
    "                break\n",
    "        if len(patches) >= max_patches:\n",
    "            break\n",
    "    slide.close()\n",
    "    return patches\n",
    "\n",
    "\n",
    "def build_patch_index(\n",
    "    split_df: pd.DataFrame,\n",
    "    root: Path,\n",
    "    level: int = 0,\n",
    "    patch_size: int = 256,\n",
    "    stride: Optional[int] = None,\n",
    "    max_patches_per_slide: int = 200,\n",
    "    tissue_threshold: float = 0.2,\n",
    ") -> pd.DataFrame:\n",
    "    records = []\n",
    "    for _, row in split_df.iterrows():\n",
    "        slide_path = resolve_slide_path(row, root)\n",
    "        if slide_path is None:\n",
    "            continue\n",
    "        try:\n",
    "            patches = extract_patches_for_slide(\n",
    "                slide_path=slide_path,\n",
    "                label=row['target_label'],\n",
    "                level=level,\n",
    "                patch_size=patch_size,\n",
    "                stride=stride,\n",
    "                max_patches=max_patches_per_slide,\n",
    "                tissue_threshold=tissue_threshold,\n",
    "            )\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            print(f\"Skipping {row['FILE_NAME']}: {exc}\")\n",
    "            continue\n",
    "\n",
    "        for patch in patches:\n",
    "            records.append({\n",
    "                'slide_id': patch.slide_id,\n",
    "                'x': patch.x,\n",
    "                'y': patch.y,\n",
    "                'level': patch.level,\n",
    "                'size': patch.size,\n",
    "                'label': patch.label,\n",
    "                'source_path': str(patch.source_path),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame.from_records(records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81dfb1",
   "metadata": {},
   "source": [
    "",
    "",
    "### Patch extraction 데모",
    "",
    "",
    "",
    "다음 셀은 training split을 기반으로 작은 patch index를 구축하려고 시도합니다. slides가 없으면 필요한 prerequisites가 누락되었음을 보고합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not HAS_OPENSLIDE:\n",
    "    print('OpenSlide missing — skipping patch extraction demo.')\n",
    "elif not WSI_ROOT.exists():\n",
    "    print('WSI directory not found — mount slides and re-run.')\n",
    "else:\n",
    "    demo_df = train_df.head(2)\n",
    "    patch_index = build_patch_index(\n",
    "        split_df=demo_df,\n",
    "        root=WSI_ROOT,\n",
    "        level=0,\n",
    "        patch_size=256,\n",
    "        stride=256,\n",
    "        max_patches_per_slide=16,\n",
    "        tissue_threshold=0.25,\n",
    "    )\n",
    "    print('Extracted', len(patch_index), 'patches')\n",
    "    patch_index.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4574374",
   "metadata": {},
   "source": [
    "",
    "",
    "## 다음 단계",
    "",
    "",
    "",
    "* 추가 diagnostic variant가 발견되면 normalisation 규칙을 보강합니다.",
    "",
    "* downstream training scripts에서 사용할 수 있도록 split metadata(`train.csv`, `valid.csv`, `test.csv`)와 생성된 `patch_index.parquet`를 저장합니다.",
    "",
    "* 모델 입력 전에 patch-level filtering(예: blur detection, stain normalisation)을 구현합니다.",
    "",
    "* 반복적인 patch extraction을 방지하기 위해 slide-level cache를 연결합니다."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}