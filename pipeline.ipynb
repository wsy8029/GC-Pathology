{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0af1dae",
   "metadata": {},
   "source": [
    "\n",
    "# Histopathology Pipeline Prototype (2024 biopsy matches)\n",
    "\n",
    "This notebook explores the 2024 biopsy matching dataset and prepares it for training a histopathology classifier. The focus is to transform the raw diagnostic reports into machine-readable labels and outline a patch-extraction workflow for `.svs` whole-slide images (WSI).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c938b",
   "metadata": {},
   "source": [
    "\n",
    "## Objectives\n",
    "\n",
    "1. Inspect and clean the biopsy metadata exported from Excel/CSV.\n",
    "2. Consolidate heterogeneous diagnostic texts into consistent disease labels.\n",
    "3. Define a modelling target that is feasible with the available annotations.\n",
    "4. Produce stratified train/validation/test splits for downstream modelling.\n",
    "5. Sketch a patch extraction pipeline for `.svs` slides, including sanity checks for missing files and simple tissue filtering heuristics.\n",
    "\n",
    "> **Note:** The repository currently only ships the metadata CSV. The actual WSI `.svs` files need to be mounted separately before running the patch extractor cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86851db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_PATH = Path('Data') / '조직검사 결과 매칭(2024)_utf8_pruned.csv'\n",
    "assert DATA_PATH.exists(), f\"Missing dataset: {DATA_PATH}\"\n",
    "\n",
    "raw_df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Loaded {len(raw_df):,} biopsy records with {raw_df.shape[1]} columns\")\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76059c2",
   "metadata": {},
   "source": [
    "### Missing value overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ca6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing_summary = (\n",
    "    raw_df.isna()\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .to_frame(name='missing_ratio')\n",
    ")\n",
    "missing_summary.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078be220",
   "metadata": {},
   "source": [
    "\n",
    "## Diagnosis normalisation\n",
    "\n",
    "The `DIAGNOSIS` column contains verbose free-text strings. To use these as machine learning targets, we:\n",
    "\n",
    "1. Keep only the portion before the first comma (drops staging or margin comments).\n",
    "2. Strip parentheses and normalise whitespace.\n",
    "3. Lowercase and map common synonyms to a canonical form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5aeeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NORMALISATION_RULES = [\n",
    "    (r'^mast cell tumor', 'mast cell tumor'),\n",
    "    (r'^cutaneous mast cell tumor', 'mast cell tumor'),\n",
    "    (r'^subcutaneous mast cell tumor', 'mast cell tumor'),\n",
    "    (r'^mammary gland adenoma', 'mammary adenoma'),\n",
    "    (r'^mammary complex adenoma', 'mammary complex adenoma'),\n",
    "    (r'^mammary benign mixed tumor', 'mammary benign mixed tumor'),\n",
    "    (r'^mammary carcinoma', 'mammary carcinoma'),\n",
    "    (r'^mammary duct carcinoma', 'mammary carcinoma'),\n",
    "    (r'^mammary adenoma', 'mammary adenoma'),\n",
    "    (r'^lipoma', 'lipoma'),\n",
    "    (r'^subcutaneous lipoma', 'lipoma'),\n",
    "    (r'^hepatic lipoma', 'lipoma'),\n",
    "    (r'^sebaceous adenoma', 'sebaceous adenoma'),\n",
    "    (r'^sebaceous epithelioma', 'sebaceous epithelioma'),\n",
    "    (r'^soft tissue sarcoma', 'soft tissue sarcoma'),\n",
    "    (r'^trichoblastoma', 'trichoblastoma'),\n",
    "]\n",
    "\n",
    "\n",
    "def normalise_diagnosis(value: str) -> str:\n",
    "    if not isinstance(value, str) or not value.strip():\n",
    "        return 'unknown'\n",
    "\n",
    "    base = value.split(',')[0]\n",
    "    base = re.sub(r'\\([^)]*\\)', '', base)\n",
    "    base = re.sub(r'[^a-zA-Z0-9\\s]', ' ', base)\n",
    "    base = re.sub(r'\\s+', ' ', base).strip().lower()\n",
    "\n",
    "    for pattern, canonical in NORMALISATION_RULES:\n",
    "        if re.match(pattern, base):\n",
    "            return canonical\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "raw_df['disease_family'] = raw_df['DIAGNOSIS'].map(normalise_diagnosis)\n",
    "print('Unique disease families:', raw_df['disease_family'].nunique())\n",
    "raw_df['disease_family'].value_counts().head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138be111",
   "metadata": {},
   "source": [
    "\n",
    "### Choosing a modelling target\n",
    "\n",
    "The dataset covers thousands of diagnostic phrases. A multi-class classifier across all 5,000+ labels would be extremely sparse. Instead we group infrequent classes into an `other` bucket and focus on the most common disease families. Adjust `MIN_CASES` to control class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a6042",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MIN_CASES = 250\n",
    "value_counts = raw_df['disease_family'].value_counts()\n",
    "major_labels = value_counts[value_counts >= MIN_CASES].index.tolist()\n",
    "print(f\"Keeping {len(major_labels)} frequent labels (≥{MIN_CASES} cases)\")\n",
    "\n",
    "raw_df['target_label'] = np.where(\n",
    "    raw_df['disease_family'].isin(major_labels),\n",
    "    raw_df['disease_family'],\n",
    "    'other'\n",
    ")\n",
    "\n",
    "raw_df['target_label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2144dc",
   "metadata": {},
   "source": [
    "\n",
    "### Train/validation/test split metadata\n",
    "\n",
    "We create stratified splits based on the consolidated target labels. These splits can later be joined with patch-level data after extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "META_COLUMNS = ['INSP_RQST_NO', 'FOLDER', 'FILE_NAME', 'target_label']\n",
    "meta_df = raw_df[META_COLUMNS].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    meta_df,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=meta_df['target_label']\n",
    ")\n",
    "valid_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=temp_df['target_label']\n",
    ")\n",
    "\n",
    "print('Train size:', len(train_df))\n",
    "print('Valid size:', len(valid_df))\n",
    "print('Test size :', len(test_df))\n",
    "\n",
    "split_summary = {\n",
    "    'train': train_df['target_label'].value_counts(normalize=True).to_dict(),\n",
    "    'valid': valid_df['target_label'].value_counts(normalize=True).to_dict(),\n",
    "    'test': test_df['target_label'].value_counts(normalize=True).to_dict(),\n",
    "}\n",
    "json.dumps(split_summary, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b0caa9",
   "metadata": {},
   "source": [
    "\n",
    "## WSI availability check\n",
    "\n",
    "The metadata references `.svs` files stored per request `FOLDER`. Update `WSI_ROOT` to the directory where slides are mounted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WSI_ROOT = Path('Data/WSI')  # TODO: update to actual location\n",
    "\n",
    "if not WSI_ROOT.exists():\n",
    "    print(f\"WSI root missing at {WSI_ROOT.resolve()}. Patch extraction will be skipped.\")\n",
    "else:\n",
    "    sample_rows = raw_df.head(3)\n",
    "    for _, row in sample_rows.iterrows():\n",
    "        candidate = WSI_ROOT / row['FOLDER'] / row['FILE_NAME']\n",
    "        print(candidate, 'exists' if candidate.exists() else 'missing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9dd9ba",
   "metadata": {},
   "source": [
    "\n",
    "## Patch extraction utilities\n",
    "\n",
    "We rely on [OpenSlide](https://openslide.org/) to stream high-resolution patches from WSI files. Install prerequisites before running the next cell:\n",
    "\n",
    "```bash\n",
    "sudo apt-get install openslide-tools\n",
    "pip install openslide-python\n",
    "```\n",
    "\n",
    "The code below degrades gracefully if OpenSlide is unavailable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f841a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import openslide\n",
    "    from PIL import Image\n",
    "    HAS_OPENSLIDE = True\n",
    "    print('OpenSlide version:', openslide.__library_version__)\n",
    "except Exception as exc:  # noqa: BLE001\n",
    "    HAS_OPENSLIDE = False\n",
    "    print('OpenSlide not available:', exc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f9e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class PatchSpec:\n",
    "    slide_id: str\n",
    "    x: int\n",
    "    y: int\n",
    "    level: int\n",
    "    size: int\n",
    "    label: str\n",
    "    source_path: Path\n",
    "\n",
    "\n",
    "def resolve_slide_path(row: pd.Series, root: Path) -> Optional[Path]:\n",
    "    \"\"\"Resolve the absolute slide path using folder/name hints.\"\"\"\n",
    "    candidates = [root / row['FOLDER'] / row['FILE_NAME'], root / row['FILE_NAME']]\n",
    "    for cand in candidates:\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "\n",
    "def tissue_fraction(tile: Image.Image) -> float:\n",
    "    arr = np.asarray(tile.convert('L'))\n",
    "    norm = (arr - arr.min()) / (arr.ptp() + 1e-6)\n",
    "    return float((norm < 0.85).mean())\n",
    "\n",
    "\n",
    "def extract_patches_for_slide(\n",
    "    slide_path: Path,\n",
    "    label: str,\n",
    "    level: int = 0,\n",
    "    patch_size: int = 256,\n",
    "    stride: Optional[int] = None,\n",
    "    max_patches: int = 200,\n",
    "    tissue_threshold: float = 0.2,\n",
    ") -> List[PatchSpec]:\n",
    "    if not HAS_OPENSLIDE:\n",
    "        raise RuntimeError('OpenSlide support is required to extract patches.')\n",
    "\n",
    "    slide = openslide.OpenSlide(str(slide_path))\n",
    "    stride = stride or patch_size\n",
    "    width, height = slide.level_dimensions[level]\n",
    "\n",
    "    patches: List[PatchSpec] = []\n",
    "    for y in range(0, height - patch_size + 1, stride):\n",
    "        for x in range(0, width - patch_size + 1, stride):\n",
    "            region = slide.read_region((x, y), level, (patch_size, patch_size))\n",
    "            if tissue_fraction(region) < tissue_threshold:\n",
    "                continue\n",
    "            patches.append(PatchSpec(\n",
    "                slide_id=slide_path.stem,\n",
    "                x=x,\n",
    "                y=y,\n",
    "                level=level,\n",
    "                size=patch_size,\n",
    "                label=label,\n",
    "                source_path=slide_path,\n",
    "            ))\n",
    "            if len(patches) >= max_patches:\n",
    "                break\n",
    "        if len(patches) >= max_patches:\n",
    "            break\n",
    "    slide.close()\n",
    "    return patches\n",
    "\n",
    "\n",
    "def build_patch_index(\n",
    "    split_df: pd.DataFrame,\n",
    "    root: Path,\n",
    "    level: int = 0,\n",
    "    patch_size: int = 256,\n",
    "    stride: Optional[int] = None,\n",
    "    max_patches_per_slide: int = 200,\n",
    "    tissue_threshold: float = 0.2,\n",
    ") -> pd.DataFrame:\n",
    "    records = []\n",
    "    for _, row in split_df.iterrows():\n",
    "        slide_path = resolve_slide_path(row, root)\n",
    "        if slide_path is None:\n",
    "            continue\n",
    "        try:\n",
    "            patches = extract_patches_for_slide(\n",
    "                slide_path=slide_path,\n",
    "                label=row['target_label'],\n",
    "                level=level,\n",
    "                patch_size=patch_size,\n",
    "                stride=stride,\n",
    "                max_patches=max_patches_per_slide,\n",
    "                tissue_threshold=tissue_threshold,\n",
    "            )\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            print(f\"Skipping {row['FILE_NAME']}: {exc}\")\n",
    "            continue\n",
    "\n",
    "        for patch in patches:\n",
    "            records.append({\n",
    "                'slide_id': patch.slide_id,\n",
    "                'x': patch.x,\n",
    "                'y': patch.y,\n",
    "                'level': patch.level,\n",
    "                'size': patch.size,\n",
    "                'label': patch.label,\n",
    "                'source_path': str(patch.source_path),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame.from_records(records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81dfb1",
   "metadata": {},
   "source": [
    "\n",
    "### Patch extraction demo\n",
    "\n",
    "The following cell attempts to build a small patch index on the training split. When slides are unavailable, it simply reports the missing prerequisites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not HAS_OPENSLIDE:\n",
    "    print('OpenSlide missing — skipping patch extraction demo.')\n",
    "elif not WSI_ROOT.exists():\n",
    "    print('WSI directory not found — mount slides and re-run.')\n",
    "else:\n",
    "    demo_df = train_df.head(2)\n",
    "    patch_index = build_patch_index(\n",
    "        split_df=demo_df,\n",
    "        root=WSI_ROOT,\n",
    "        level=0,\n",
    "        patch_size=256,\n",
    "        stride=256,\n",
    "        max_patches_per_slide=16,\n",
    "        tissue_threshold=0.25,\n",
    "    )\n",
    "    print('Extracted', len(patch_index), 'patches')\n",
    "    patch_index.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4574374",
   "metadata": {},
   "source": [
    "\n",
    "## Next steps\n",
    "\n",
    "* Augment the normalisation rules once additional diagnostic variants are observed.\n",
    "* Persist the split metadata (`train.csv`, `valid.csv`, `test.csv`) and the generated `patch_index.parquet` for downstream training scripts.\n",
    "* Implement patch-level filtering (e.g. blur detection, stain normalisation) prior to model ingestion.\n",
    "* Connect a slide-level cache so that repeated patch extraction is avoided during experimentation.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
