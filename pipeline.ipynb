{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0af1dae",
   "metadata": {},
   "source": [
    "# Histopathology Pipeline Prototype (2024년 Biopsy Matches)\n",
    "\n",
    "이 노트북은 2024년 biopsy matching 데이터셋을 탐색하고 histopathology classifier 학습을 준비하는 전체 흐름을 정리한 문서입니다. raw diagnostic report를 machine-readable label로 정제하고, `.svs` whole-slide image(WSI)를 기반으로 한 patch extraction 절차를 개괄하는 것이 핵심입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c938b",
   "metadata": {},
   "source": [
    "## 목표\n",
    "\n",
    "1. Excel/CSV에서 export된 biopsy metadata를 점검하고 정제합니다.\n",
    "2. 이질적인 diagnostic 텍스트를 일관된 disease label로 통합합니다.\n",
    "3. 이용 가능한 annotation을 바탕으로 실용적인 modelling target을 정의합니다.\n",
    "4. downstream modelling에 사용할 stratified train/validation/test split을 생성합니다.\n",
    "5. `.svs` slide에 대한 patch extraction workflow를 개괄하고, 누락된 파일 여부와 tissue filtering heuristic을 설명합니다.\n",
    "\n",
    "> **참고:** 현재 레포지토리에는 metadata CSV만 포함되어 있습니다. 실제 WSI `.svs` 파일은 patch extractor 셀을 실행하기 전에 별도로 마운트해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86851db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# --- 데이터 적재 및 기본 환경 설정 ---\n",
    "# 공통 모듈을 한 번에 import하고, 이후 단계에서 반복 사용되는 기본 DataFrame을 메모리에 적재합니다.\n",
    "# 이렇게 구성하면 노트북 전체에서 동일한 상태를 공유하며 중복 I/O를 줄일 수 있습니다.\n",
    "\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_PATH = Path('Data') / '조직검사 결과 매칭(2024)_utf8_pruned.csv'\n",
    "# CSV 경로를 상수로 두어 실험 환경에 따라 경로만 교체하면 되도록 했습니다.\n",
    "assert DATA_PATH.exists(), f\"Missing dataset: {DATA_PATH}\"\n",
    "# 조기 실패를 통해 경로 설정 오류를 빠르게 발견하고 후속 셀의 예외 연쇄를 막습니다.\n",
    "\n",
    "raw_df = pd.read_csv(DATA_PATH)\n",
    "# 모든 후속 통계와 split 단계가 동일한 원본 뷰에서 출발하도록 raw_df를 단일 소스로 유지합니다.\n",
    "print(f\"Loaded {len(raw_df):,} biopsy records with {raw_df.shape[1]} columns\")\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76059c2",
   "metadata": {},
   "source": [
    "### 결측값 개요\n",
    "\n",
    "이 단계에서는 `raw_df`의 모든 컬럼에 대해 결측값 비율을 계산하고 내림차순으로 정렬합니다. 데이터 품질을 빠르게 점검하고, 추가 정제가 필요한 필드를 선별하기 위한 사전 작업입니다.\n",
    "\n",
    "셀을 실행하면 결측 비율이 가장 높은 컬럼부터 나열된 `Series`가 출력됩니다. 상위 항목을 확인해 결측이 심한 열을 별도로 보강할지, 분석에서 제외할지 판단하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ca6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 결측값 진단으로 전처리 우선순위 설정 ---\n",
    "# 각 컬럼의 결측 비율을 계산해 downstream 단계에서 먼저 다듬어야 할 필드를 빠르게 파악합니다.\n",
    "# 비율 기반 정렬을 사용하면 전체 데이터 크기에 상관없이 상대적 우선순위를 즉시 확인할 수 있습니다.\n",
    "\n",
    "missing_summary = (\n",
    "    raw_df.isna()\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .to_frame(name='missing_ratio')\n",
    ")\n",
    "# 상위 열만 확인해도 주요 결측 패턴을 파악할 수 있어 탐색 시간을 단축합니다.\n",
    "missing_summary.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078be220",
   "metadata": {},
   "source": [
    "## Diagnosis 정규화\n",
    "\n",
    "`DIAGNOSIS` 컬럼에는 장문의 free-text 문자열이 포함되어 있어 모델 학습에 바로 활용하기 어렵습니다. 아래 단계는 텍스트를 간결한 label로 변환하여 클래스 수를 제어하고, downstream task에서 일관된 정답을 제공하기 위한 전처리입니다.\n",
    "\n",
    "1. 첫 번째 쉼표 이전 부분만 유지하여 staging·margin과 같은 부가 설명을 제거합니다.\n",
    "2. 괄호나 중복 공백을 제거해 문자열 패턴을 단순화합니다.\n",
    "3. 소문자로 변환한 뒤 자주 등장하는 동의어를 canonical form으로 매핑합니다.\n",
    "\n",
    "코드를 실행하면 규칙 기반 매핑이 적용된 `raw_df['target_label']`이 생성됩니다. 각 규칙의 적용 결과를 검토해 잘못된 매핑이 없는지 확인하고, 필요 시 목록을 보완하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5aeeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 규칙 기반 진단명 정규화 ---\n",
    "# 질병명을 사전 정의된 canonical label로 매핑해 희귀 변형을 통합하고 통계적 안정성을 확보합니다.\n",
    "# 정규표현식을 사용하면 접두어나 수식어가 추가된 케이스도 동일 로직으로 관리할 수 있습니다.\n",
    "\n",
    "NORMALISATION_RULES = [\n",
    "    (r'^mast cell tumor', 'mast cell tumor'),\n",
    "    (r'^cutaneous mast cell tumor', 'mast cell tumor'),\n",
    "    (r'^subcutaneous mast cell tumor', 'mast cell tumor'),\n",
    "    (r'^mammary gland adenoma', 'mammary adenoma'),\n",
    "    (r'^mammary complex adenoma', 'mammary complex adenoma'),\n",
    "    (r'^mammary benign mixed tumor', 'mammary benign mixed tumor'),\n",
    "    (r'^mammary carcinoma', 'mammary carcinoma'),\n",
    "    (r'^mammary duct carcinoma', 'mammary carcinoma'),\n",
    "    (r'^mammary adenoma', 'mammary adenoma'),\n",
    "    (r'^lipoma', 'lipoma'),\n",
    "    (r'^subcutaneous lipoma', 'lipoma'),\n",
    "    (r'^hepatic lipoma', 'lipoma'),\n",
    "    (r'^sebaceous adenoma', 'sebaceous adenoma'),\n",
    "    (r'^sebaceous epithelioma', 'sebaceous epithelioma'),\n",
    "    (r'^soft tissue sarcoma', 'soft tissue sarcoma'),\n",
    "    (r'^trichoblastoma', 'trichoblastoma'),\n",
    "]\n",
    "\n",
    "\n",
    "def normalise_diagnosis(value: str) -> str:\n",
    "    if not isinstance(value, str) or not value.strip():\n",
    "        return 'unknown'\n",
    "\n",
    "    # 쉼표 앞부분만 남겨 병기/절제여부 등 보조 정보를 제거해 핵심 진단명에 집중합니다.\n",
    "    base = value.split(',')[0]\n",
    "    # 괄호와 특수문자를 걷어내 텍스트 패턴을 단순화하고 규칙 재사용성을 높입니다.\n",
    "    base = re.sub(r'\\([^)]*\\)', '', base)\n",
    "    base = re.sub(r'[^a-zA-Z0-9\\s]', ' ', base)\n",
    "    base = re.sub(r'\\s+', ' ', base).strip().lower()\n",
    "\n",
    "    for pattern, canonical in NORMALISATION_RULES:\n",
    "        if re.match(pattern, base):\n",
    "            return canonical\n",
    "\n",
    "    # 사전에 없는 경우에는 정제된 문자열을 그대로 반환해 신규 케이스를 추적할 수 있도록 합니다.\n",
    "    return base\n",
    "\n",
    "\n",
    "raw_df['disease_family'] = raw_df['DIAGNOSIS'].map(normalise_diagnosis)\n",
    "# 정규화된 family 레벨을 별도 컬럼으로 유지해 threshold 조정이나 규칙 보강이 용이하도록 했습니다.\n",
    "print('Unique disease families:', raw_df['disease_family'].nunique())\n",
    "raw_df['disease_family'].value_counts().head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138be111",
   "metadata": {},
   "source": [
    "### 모델링 target 선택\n",
    "\n",
    "데이터셋에는 수천 개의 diagnostic 문구가 존재하며, 모든 label을 개별 클래스로 다루면 극도로 희소해집니다. 따라서 빈도가 낮은 클래스는 `other` 버킷으로 묶고, 가장 흔한 disease family에 집중해 안정적인 분포를 확보합니다.\n",
    "\n",
    "셀 실행 시 `MIN_CASES` 이상 등장하는 label 개수와 목록이 출력됩니다. 결과를 통해 클래스 불균형 정도를 파악하고, threshold를 조정해 원하는 granularity를 찾을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a6042",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 희소 클래스 통합 전략 ---\n",
    "# 클래스 빈도가 극단적으로 낮으면 모델 학습이 불안정해지므로, 최소 증례 수 기준을 둬 주요 질환군에 집중합니다.\n",
    "# 한 번 계산한 value_counts를 재사용해 threshold를 조정할 때도 비용이 들지 않도록 했습니다.\n",
    "\n",
    "MIN_CASES = 250\n",
    "value_counts = raw_df['disease_family'].value_counts()\n",
    "major_labels = value_counts[value_counts >= MIN_CASES].index.tolist()\n",
    "print(f\"Keeping {len(major_labels)} frequent labels (≥{MIN_CASES} cases)\")\n",
    "\n",
    "raw_df['target_label'] = np.where(\n",
    "    raw_df['disease_family'].isin(major_labels),\n",
    "    raw_df['disease_family'],\n",
    "    'other'\n",
    ")\n",
    "# 희귀 클래스는 'other'로 묶어 추후 분석 시 다시 drill-down 할 수 있는 최소 정보를 남겨둡니다.\n",
    "\n",
    "raw_df['target_label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2144dc",
   "metadata": {},
   "source": [
    "### Train/validation/test split 메타데이터\n",
    "\n",
    "정규화된 target label을 기준으로 stratified split을 생성하여 추후 patch-level 데이터와 결합합니다. 동일 환자·슬라이드가 여러 split에 중복되지 않도록 메타데이터 키(`INSP_RQST_NO`, `FOLDER`, `FILE_NAME`)를 유지합니다.\n",
    "\n",
    "코드를 실행하면 `train_df`, `valid_df`, `test_df`가 만들어지고, 각 데이터프레임의 크기가 로그로 표시됩니다. 분할된 데이터는 모델 학습에서 재현성을 보장하고, validation/test 평가를 공정하게 유지하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Stratified 데이터 분할 ---\n",
    "# 동일한 환자/슬라이드가 여러 split에 중복되지 않도록 키 컬럼만 남겨 안정적인 평가 집합을 만듭니다.\n",
    "# Stratified split을 통해 label 분포를 최대한 유지해 모델 튜닝 시 분산을 줄입니다.\n",
    "\n",
    "META_COLUMNS = ['INSP_RQST_NO', 'FOLDER', 'FILE_NAME', 'target_label']\n",
    "meta_df = raw_df[META_COLUMNS].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    meta_df,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=meta_df['target_label']\n",
    ")\n",
    "valid_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=temp_df['target_label']\n",
    ")\n",
    "\n",
    "print('Train size:', len(train_df))\n",
    "print('Valid size:', len(valid_df))\n",
    "print('Test size :', len(test_df))\n",
    "\n",
    "split_summary = {\n",
    "    'train': train_df['target_label'].value_counts(normalize=True).to_dict(),\n",
    "    'valid': valid_df['target_label'].value_counts(normalize=True).to_dict(),\n",
    "    'test': test_df['target_label'].value_counts(normalize=True).to_dict(),\n",
    "}\n",
    "# JSON 구조로 직렬화하면 버전 관리나 외부 파이프라인 연동 시 재사용이 수월합니다.\n",
    "json.dumps(split_summary, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b0caa9",
   "metadata": {},
   "source": [
    "## WSI 가용성 확인\n",
    "\n",
    "metadata는 요청별 `FOLDER`에 저장된 `.svs` 파일을 참조합니다. 실제 슬라이드 파일이 마운트된 디렉터리 경로를 `WSI_ROOT`에 지정해 존재 여부를 점검합니다.\n",
    "\n",
    "경로가 존재하지 않으면 경고 메시지가 출력되며, patch extraction 단계가 자동으로 건너뛰어집니다. 반대로 슬라이드가 감지되면 이후 셀에서 곧바로 patch를 추출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 슬라이드 파일 가용성 점검 ---\n",
    "# metadata에서 참조하는 경로 구조를 실제 파일 시스템과 비교해, 추후 patch 추출 단계에서 발생할 실패를 미리 차단합니다.\n",
    "\n",
    "WSI_ROOT = Path('Data/WSI')  # TODO: update to actual location\n",
    "\n",
    "if not WSI_ROOT.exists():\n",
    "    print(f\"WSI root missing at {WSI_ROOT.resolve()}. Patch extraction will be skipped.\")\n",
    "else:\n",
    "    sample_rows = raw_df.head(3)\n",
    "    for _, row in sample_rows.iterrows():\n",
    "        candidate = WSI_ROOT / row['FOLDER'] / row['FILE_NAME']\n",
    "        # 샘플 경로를 출력해 슬라이드 구조가 예상과 일치하는지 수동으로 확인할 수 있게 합니다.\n",
    "        print(candidate, 'exists' if candidate.exists() else 'missing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9dd9ba",
   "metadata": {},
   "source": [
    "## Patch extraction 유틸리티\n",
    "\n",
    "고해상도 패치를 스트리밍하기 위해 [OpenSlide](https://openslide.org/)를 활용합니다. prerequisites는 다음과 같습니다.\n",
    "\n",
    "```bash\n",
    "sudo apt-get install openslide-tools\n",
    "pip install openslide-python\n",
    "```\n",
    "\n",
    "라이브러리가 정상적으로 import되면 `HAS_OPENSLIDE` 플래그가 `True`로 설정되고, 이후 셀에서 실제 patch extraction 함수가 작동합니다. 설치가 되지 않았을 경우 우회 메시지를 출력해 추후 조치를 안내합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f841a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- OpenSlide 의존성 로딩 ---\n",
    "# WSI 스트리밍은 외부 C 라이브러리에 의존하므로, import 단계에서 명시적으로 가용성을 점검해 이후 셀의 예외를 최소화합니다.\n",
    "\n",
    "try:\n",
    "    import openslide\n",
    "    from PIL import Image\n",
    "    HAS_OPENSLIDE = True\n",
    "    print('OpenSlide version:', openslide.__library_version__)\n",
    "except Exception as exc:  # noqa: BLE001\n",
    "    HAS_OPENSLIDE = False\n",
    "    print('OpenSlide not available:', exc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f9e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Patch 추출 유틸리티 정의 ---\n",
    "# 추후 재현 가능한 patch index를 만들기 위해 좌표, 레벨, 레이블 정보를 하나의 dataclass로 묶어 구조화합니다.\n",
    "# 각 함수에는 실패 지점을 조기에 드러내기 위한 방어 로직을 추가해 대규모 배치 실행 시 디버깅 비용을 줄입니다.\n",
    "\n",
    "@dataclass\n",
    "class PatchSpec:\n",
    "    slide_id: str\n",
    "    x: int\n",
    "    y: int\n",
    "    level: int\n",
    "    size: int\n",
    "    label: str\n",
    "    source_path: Path\n",
    "\n",
    "\n",
    "def resolve_slide_path(row: pd.Series, root: Path) -> Optional[Path]:\n",
    "    \"\"\"Resolve the absolute slide path using folder/name hints.\"\"\"\n",
    "    # 일부 항목은 폴더 구조가 다를 수 있으므로, 여러 후보 경로를 순차적으로 검사해 누락을 최소화합니다.\n",
    "    candidates = [root / row['FOLDER'] / row['FILE_NAME'], root / row['FILE_NAME']]\n",
    "    for cand in candidates:\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "\n",
    "def tissue_fraction(tile: Image.Image) -> float:\n",
    "    # 간단한 intensity 기반 휴리스틱으로 배경을 제거해, 계산 비용이 큰 deep model을 호출하기 전 1차 필터링을 수행합니다.\n",
    "    arr = np.asarray(tile.convert('L'))\n",
    "    norm = (arr - arr.min()) / (arr.ptp() + 1e-6)\n",
    "    return float((norm < 0.85).mean())\n",
    "\n",
    "\n",
    "def extract_patches_for_slide(\n",
    "    slide_path: Path,\n",
    "    label: str,\n",
    "    level: int = 0,\n",
    "    patch_size: int = 256,\n",
    "    stride: Optional[int] = None,\n",
    "    max_patches: int = 200,\n",
    "    tissue_threshold: float = 0.2,\n",
    ") -> List[PatchSpec]:\n",
    "    if not HAS_OPENSLIDE:\n",
    "        raise RuntimeError('OpenSlide support is required to extract patches.')\n",
    "\n",
    "    slide = openslide.OpenSlide(str(slide_path))\n",
    "    stride = stride or patch_size\n",
    "    width, height = slide.level_dimensions[level]\n",
    "\n",
    "    patches: List[PatchSpec] = []\n",
    "    for y in range(0, height - patch_size + 1, stride):\n",
    "        for x in range(0, width - patch_size + 1, stride):\n",
    "            region = slide.read_region((x, y), level, (patch_size, patch_size))\n",
    "            # tissue_fraction이 낮으면 배경 패치이므로 스토리지와 학습 시간을 아끼기 위해 건너뜁니다.\n",
    "            if tissue_fraction(region) < tissue_threshold:\n",
    "                continue\n",
    "            patches.append(PatchSpec(\n",
    "                slide_id=slide_path.stem,\n",
    "                x=x,\n",
    "                y=y,\n",
    "                level=level,\n",
    "                size=patch_size,\n",
    "                label=label,\n",
    "                source_path=slide_path,\n",
    "            ))\n",
    "            if len(patches) >= max_patches:\n",
    "                break\n",
    "        if len(patches) >= max_patches:\n",
    "            break\n",
    "    slide.close()\n",
    "    return patches\n",
    "\n",
    "\n",
    "def build_patch_index(\n",
    "    split_df: pd.DataFrame,\n",
    "    root: Path,\n",
    "    level: int = 0,\n",
    "    patch_size: int = 256,\n",
    "    stride: Optional[int] = None,\n",
    "    max_patches_per_slide: int = 200,\n",
    "    tissue_threshold: float = 0.2,\n",
    ") -> pd.DataFrame:\n",
    "    records = []\n",
    "    for _, row in split_df.iterrows():\n",
    "        slide_path = resolve_slide_path(row, root)\n",
    "        if slide_path is None:\n",
    "            continue\n",
    "        try:\n",
    "            patches = extract_patches_for_slide(\n",
    "                slide_path=slide_path,\n",
    "                label=row['target_label'],\n",
    "                level=level,\n",
    "                patch_size=patch_size,\n",
    "                stride=stride,\n",
    "                max_patches=max_patches_per_slide,\n",
    "                tissue_threshold=tissue_threshold,\n",
    "            )\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            # 실패한 슬라이드는 로깅만 하고 계속 진행해 배치 실행이 중단되지 않도록 합니다.\n",
    "            print(f\"Skipping {row['FILE_NAME']}: {exc}\")\n",
    "            continue\n",
    "\n",
    "        for patch in patches:\n",
    "            records.append({\n",
    "                'slide_id': patch.slide_id,\n",
    "                'x': patch.x,\n",
    "                'y': patch.y,\n",
    "                'level': patch.level,\n",
    "                'size': patch.size,\n",
    "                'label': patch.label,\n",
    "                'source_path': str(patch.source_path),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame.from_records(records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81dfb1",
   "metadata": {},
   "source": [
    "### Patch extraction 데모\n",
    "\n",
    "이 셀은 training split에 속한 슬라이드 중 일부를 대상으로 작은 patch index를 구축하는 예시입니다. 실제 파일과 OpenSlide 환경이 갖춰져 있다면 지정된 좌표와 크기에 맞춰 패치를 추출하고, 간단한 품질 필터를 적용해 parquet 파일로 저장하는 흐름을 시연합니다.\n",
    "\n",
    "실행 결과:\n",
    "- OpenSlide 또는 슬라이드 디렉터리가 없으면 원인을 설명하는 메시지가 출력됩니다.\n",
    "- 모든 준비가 되어 있으면 추출된 패치 수와 저장 경로가 로그로 표시됩니다. 이를 통해 pipeline 구동 여부를 즉시 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Patch extraction 데모 실행 제어 ---\n",
    "# 환경 준비 여부에 따라 실행을 분기해, 의존성이 없을 때도 노트북 전체 흐름을 중단하지 않고 안내 메시지를 제공합니다.\n",
    "\n",
    "if not HAS_OPENSLIDE:\n",
    "    print('OpenSlide missing — skipping patch extraction demo.')\n",
    "elif not WSI_ROOT.exists():\n",
    "    print('WSI directory not found — mount slides and re-run.')\n",
    "else:\n",
    "    demo_df = train_df.head(2)\n",
    "    patch_index = build_patch_index(\n",
    "        split_df=demo_df,\n",
    "        root=WSI_ROOT,\n",
    "        level=0,\n",
    "        patch_size=256,\n",
    "        stride=256,\n",
    "        max_patches_per_slide=16,\n",
    "        tissue_threshold=0.25,\n",
    "    )\n",
    "    print('Extracted', len(patch_index), 'patches')\n",
    "    patch_index.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4574374",
   "metadata": {},
   "source": [
    "## 다음 단계\n",
    "\n",
    "* 새로운 diagnostic variant가 발견되면 정규화 규칙을 업데이트하여 label 누락을 방지합니다.\n",
    "* downstream training script에서 사용할 수 있도록 split metadata(`train.csv`, `valid.csv`, `test.csv`)와 생성된 `patch_index.parquet`를 버전 관리합니다.\n",
    "* 모델 입력 전에 patch-level filtering(예: blur detection, stain normalisation)을 적용해 데이터 품질을 향상시킵니다.\n",
    "* 반복적인 patch extraction을 방지하도록 slide-level cache 또는 artifact 저장 전략을 마련합니다."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
